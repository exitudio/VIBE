{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import joblib\n",
    "import shutil\n",
    "import colorsys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multi_person_tracker import MPT\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lib.models.vibe import VIBE_Demo\n",
    "from lib.utils.renderer import Renderer\n",
    "from lib.dataset.inference import Inference\n",
    "from lib.utils.smooth_pose import smooth_pose\n",
    "from lib.data_utils.kp_utils import convert_kps\n",
    "from lib.utils.pose_tracker import run_posetracker\n",
    "\n",
    "from lib.utils.demo_utils import (\n",
    "    download_youtube_clip,\n",
    "    smplify_runner,\n",
    "    convert_crop_coords_to_orig_img,\n",
    "    convert_crop_cam_to_orig_img,\n",
    "    prepare_rendering_results,\n",
    "    video_to_images,\n",
    "    images_to_video,\n",
    "    download_ckpt,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ml_collections\n",
    "MIN_NUM_FRAMES = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "1. args.tracking_method == 'pose'<br>\n",
    "2. download_ckpt(use_3dpw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ml_collections.ConfigDict()\n",
    "args.tracker_batch_size = 12\n",
    "args.display = False # need to be false for jupyter\n",
    "args.detector = 'yolo'\n",
    "args.yolo_img_size = 416\n",
    "args.tracking_method = 'bbox'\n",
    "args.vibe_batch_size = 450\n",
    "args.wireframe = False\n",
    "\n",
    "\n",
    "# for args.tracking_method == 'pose'\n",
    "# staf_dir = '/home/mkocabas/developments/openposetrack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \"ffmpeg -i /home/epinyoan/dataset/casia-b/dataset_b/Datset-B-1/video/052-nm-03-144.avi -f image2 -v error ./output/sample_video/052-nm-03-144_avi/%06d.png\"\n",
      "Images saved to \"./output/sample_video/052-nm-03-144_avi\"\n",
      "Input video number of frames 101\n",
      "Running Multi-Person-Tracker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Detection + Tracking FPS 56.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "video_file = '/home/epinyoan/dataset/casia-b/dataset_b/Datset-B-1/video/052-nm-03-144.avi'\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "image_folder, num_frames, img_shape = video_to_images(video_file, return_info=True)\n",
    "print(f'Input video number of frames {num_frames}')\n",
    "orig_height, orig_width = img_shape[:2]\n",
    "total_time = time.time()\n",
    "\n",
    "bbox_scale = 1.1\n",
    "# run multi object tracker\n",
    "mot = MPT(\n",
    "    device=device,\n",
    "    batch_size=args.tracker_batch_size,\n",
    "    display=args.display,\n",
    "    detector_type=args.detector,\n",
    "    output_format='dict',\n",
    "    yolo_img_size=args.yolo_img_size,\n",
    ")\n",
    "tracking_results = mot(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "=> loaded pretrained model from 'data/vibe_data/spin_model_checkpoint.pth.tar'\n",
      "Performance of pretrained model on 3DPW: 52.00081691145897\n",
      "Loaded pretrained weights from \"data/vibe_data/vibe_model_w_3dpw.pth.tar\"\n"
     ]
    }
   ],
   "source": [
    "# remove tracklets if num_frames is less than MIN_NUM_FRAMES\n",
    "for person_id in list(tracking_results.keys()):\n",
    "    if tracking_results[person_id]['frames'].shape[0] < MIN_NUM_FRAMES:\n",
    "        del tracking_results[person_id]\n",
    "\n",
    "# ========= Define VIBE model ========= #\n",
    "model = VIBE_Demo(\n",
    "    seqlen=16,\n",
    "    n_layers=2,\n",
    "    hidden_size=1024,\n",
    "    add_linear=True,\n",
    "    use_residual=True,\n",
    ").to(device)\n",
    "\n",
    "# ========= Load pretrained weights ========= #\n",
    "pretrained_file = download_ckpt(use_3dpw=True)\n",
    "ckpt = torch.load(pretrained_file)\n",
    "print(f'Performance of pretrained model on 3DPW: {ckpt[\"performance\"]}')\n",
    "ckpt = ckpt['gen_state_dict']\n",
    "model.load_state_dict(ckpt, strict=False)\n",
    "model.eval()\n",
    "print(f'Loaded pretrained weights from \\\"{pretrained_file}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running VIBE on each tracklet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# ========= Run VIBE on each person ========= #\n",
    "print(f'Running VIBE on each tracklet...')\n",
    "vibe_time = time.time()\n",
    "vibe_results = {}\n",
    "for person_id in tqdm(list(tracking_results.keys())):\n",
    "    bboxes = joints2d = None\n",
    "\n",
    "    if args.tracking_method == 'bbox':\n",
    "        bboxes = tracking_results[person_id]['bbox']\n",
    "    elif args.tracking_method == 'pose':\n",
    "        joints2d = tracking_results[person_id]['joints2d']\n",
    "\n",
    "    frames = tracking_results[person_id]['frames']\n",
    "\n",
    "    dataset = Inference(\n",
    "        image_folder=image_folder,\n",
    "        frames=frames,\n",
    "        bboxes=bboxes,\n",
    "        joints2d=joints2d,\n",
    "        scale=bbox_scale,\n",
    "    )\n",
    "\n",
    "    bboxes = dataset.bboxes\n",
    "    frames = dataset.frames\n",
    "    has_keypoints = True if joints2d is not None else False\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=args.vibe_batch_size, num_workers=16)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred_cam, pred_verts, pred_pose, pred_betas, pred_joints3d, smpl_joints2d, norm_joints2d = [], [], [], [], [], [], []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            if has_keypoints:\n",
    "                batch, nj2d = batch\n",
    "                norm_joints2d.append(nj2d.numpy().reshape(-1, 21, 3))\n",
    "\n",
    "            batch = batch.unsqueeze(0)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            batch_size, seqlen = batch.shape[:2]\n",
    "            output = model(batch)[-1]\n",
    "\n",
    "            pred_cam.append(output['theta'][:, :, :3].reshape(batch_size * seqlen, -1))\n",
    "            pred_verts.append(output['verts'].reshape(batch_size * seqlen, -1, 3))\n",
    "            pred_pose.append(output['theta'][:,:,3:75].reshape(batch_size * seqlen, -1))\n",
    "            pred_betas.append(output['theta'][:, :,75:].reshape(batch_size * seqlen, -1))\n",
    "            pred_joints3d.append(output['kp_3d'].reshape(batch_size * seqlen, -1, 3))\n",
    "            smpl_joints2d.append(output['kp_2d'].reshape(batch_size * seqlen, -1, 2))\n",
    "\n",
    "\n",
    "        pred_cam = torch.cat(pred_cam, dim=0)\n",
    "        pred_verts = torch.cat(pred_verts, dim=0)\n",
    "        pred_pose = torch.cat(pred_pose, dim=0)\n",
    "        pred_betas = torch.cat(pred_betas, dim=0)\n",
    "        pred_joints3d = torch.cat(pred_joints3d, dim=0)\n",
    "        smpl_joints2d = torch.cat(smpl_joints2d, dim=0)\n",
    "        del batch\n",
    "\n",
    "        # ========= Save results to a pickle file ========= #\n",
    "        pred_cam = pred_cam.cpu().numpy()\n",
    "        pred_verts = pred_verts.cpu().numpy()\n",
    "        pred_pose = pred_pose.cpu().numpy()\n",
    "        pred_betas = pred_betas.cpu().numpy()\n",
    "        pred_joints3d = pred_joints3d.cpu().numpy()\n",
    "        smpl_joints2d = smpl_joints2d.cpu().numpy()\n",
    "\n",
    "        # Runs 1 Euro Filter to smooth out the results\n",
    "#         if args.smooth:\n",
    "#             min_cutoff = args.smooth_min_cutoff # 0.004\n",
    "#             beta = args.smooth_beta # 1.5\n",
    "#             print(f'Running smoothing on person {person_id}, min_cutoff: {min_cutoff}, beta: {beta}')\n",
    "#             pred_verts, pred_pose, pred_joints3d = smooth_pose(pred_pose, pred_betas,\n",
    "#                                                                min_cutoff=min_cutoff, beta=beta)\n",
    "\n",
    "        orig_cam = convert_crop_cam_to_orig_img(\n",
    "            cam=pred_cam,\n",
    "            bbox=bboxes,\n",
    "            img_width=orig_width,\n",
    "            img_height=orig_height\n",
    "        )\n",
    "\n",
    "        joints2d_img_coord = convert_crop_coords_to_orig_img(\n",
    "            bbox=bboxes,\n",
    "            keypoints=smpl_joints2d,\n",
    "            crop_size=224,\n",
    "        )\n",
    "\n",
    "        output_dict = {\n",
    "            'pred_cam': pred_cam,\n",
    "            'orig_cam': orig_cam,\n",
    "            'verts': pred_verts,\n",
    "            'pose': pred_pose,\n",
    "            'betas': pred_betas,\n",
    "            'joints3d': pred_joints3d,\n",
    "            'joints2d': joints2d,\n",
    "            'joints2d_img_coord': joints2d_img_coord,\n",
    "            'bboxes': bboxes,\n",
    "            'frame_ids': frames,\n",
    "        }\n",
    "\n",
    "        vibe_results[person_id] = output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████▊                                                        | 31/101 [00:00<00:00, 189.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "(240, 320, 3)\n",
      "30\n",
      "(240, 320, 3)\n",
      "31\n",
      "(240, 320, 3)\n",
      "32\n",
      "(240, 320, 3)\n",
      "33\n",
      "(240, 320, 3)\n",
      "34\n",
      "(240, 320, 3)\n",
      "35\n",
      "(240, 320, 3)\n",
      "36\n",
      "(240, 320, 3)\n",
      "37\n",
      "(240, 320, 3)\n",
      "38\n",
      "(240, 320, 3)\n",
      "39\n",
      "(240, 320, 3)\n",
      "40\n",
      "(240, 320, 3)\n",
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████▌                                         | 50/101 [00:00<00:00, 76.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 3)\n",
      "42\n",
      "(240, 320, 3)\n",
      "43\n",
      "(240, 320, 3)\n",
      "44\n",
      "(240, 320, 3)\n",
      "45\n",
      "(240, 320, 3)\n",
      "46\n",
      "(240, 320, 3)\n",
      "47\n",
      "(240, 320, 3)\n",
      "48\n",
      "(240, 320, 3)\n",
      "49\n",
      "(240, 320, 3)\n",
      "50\n",
      "(240, 320, 3)\n",
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▌                                | 61/101 [00:00<00:00, 62.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 3)\n",
      "52\n",
      "(240, 320, 3)\n",
      "53\n",
      "(240, 320, 3)\n",
      "54\n",
      "(240, 320, 3)\n",
      "55\n",
      "(240, 320, 3)\n",
      "56\n",
      "(240, 320, 3)\n",
      "57\n",
      "(240, 320, 3)\n",
      "58\n",
      "(240, 320, 3)\n",
      "59\n",
      "(240, 320, 3)\n",
      "60\n",
      "(240, 320, 3)\n",
      "61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████████████████████████████                          | 69/101 [00:01<00:00, 58.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 3)\n",
      "62\n",
      "(240, 320, 3)\n",
      "63\n",
      "(240, 320, 3)\n",
      "64\n",
      "(240, 320, 3)\n",
      "65\n",
      "(240, 320, 3)\n",
      "66\n",
      "(240, 320, 3)\n",
      "67\n",
      "(240, 320, 3)\n",
      "68\n",
      "(240, 320, 3)\n",
      "69\n",
      "(240, 320, 3)\n",
      "70\n",
      "(240, 320, 3)\n",
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▋                    | 76/101 [00:01<00:00, 55.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 3)\n",
      "72\n",
      "(240, 320, 3)\n",
      "73\n",
      "(240, 320, 3)\n",
      "74\n",
      "(240, 320, 3)\n",
      "75\n",
      "(240, 320, 3)\n",
      "76\n",
      "(240, 320, 3)\n",
      "77\n",
      "(240, 320, 3)\n",
      "78\n",
      "(240, 320, 3)\n",
      "79\n",
      "(240, 320, 3)\n",
      "80\n",
      "(240, 320, 3)\n",
      "81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████▍          | 88/101 [00:01<00:00, 51.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 3)\n",
      "82\n",
      "(240, 320, 3)\n",
      "83\n",
      "(240, 320, 3)\n",
      "84\n",
      "(240, 320, 3)\n",
      "85\n",
      "(240, 320, 3)\n",
      "86\n",
      "(240, 320, 3)\n",
      "87\n",
      "(240, 320, 3)\n",
      "88\n",
      "(240, 320, 3)\n",
      "89\n",
      "(240, 320, 3)\n",
      "90\n",
      "(240, 320, 3)\n",
      "91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████████████████████████████████████████████▏| 100/101 [00:01<00:00, 47.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 3)\n",
      "92\n",
      "(240, 320, 3)\n",
      "93\n",
      "(240, 320, 3)\n",
      "94\n",
      "(240, 320, 3)\n",
      "95\n",
      "(240, 320, 3)\n",
      "96\n",
      "(240, 320, 3)\n",
      "97\n",
      "(240, 320, 3)\n",
      "98\n",
      "(240, 320, 3)\n",
      "99\n",
      "(240, 320, 3)\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 101/101 [00:01<00:00, 58.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# prepare results for rendering\n",
    "renderer = Renderer(resolution=(orig_width, orig_height), orig_img=True, wireframe=args.wireframe)\n",
    "frame_results = prepare_rendering_results(vibe_results, num_frames)\n",
    "mesh_color = {k: colorsys.hsv_to_rgb(np.random.rand(), 0.5, 1.0) for k in vibe_results.keys()}\n",
    "image_file_names = sorted([\n",
    "    os.path.join(image_folder, x)\n",
    "    for x in os.listdir(image_folder)\n",
    "    if x.endswith('.png') or x.endswith('.jpg')\n",
    "])\n",
    "for frame_idx in tqdm(range(len(image_file_names))):\n",
    "    img_fname = image_file_names[frame_idx]\n",
    "    img = cv2.imread(img_fname)\n",
    "    print(img.shape)\n",
    "    for person_id, person_data in frame_results[frame_idx].items():\n",
    "        print(frame_idx)\n",
    "        frame_verts = person_data['verts']\n",
    "        frame_cam = person_data['cam']\n",
    "        mc = mesh_color[person_id]\n",
    "        mesh_filename = None\n",
    "        img = renderer.render(\n",
    "            img,\n",
    "            frame_verts,\n",
    "            cam=frame_cam,\n",
    "            color=[1,0,0],\n",
    "            mesh_filename=mesh_filename,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6890, 3) (13776, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epinyoan/miniconda3/envs/vibe-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1e8cc4ded0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# frame_cam_ = output['theta'][:, :, :3].reshape(batch_size * seqlen, -1)\n",
    "verts_ = output['verts'].reshape(batch_size * seqlen, -1, 3)\n",
    "img = renderer.render(\n",
    "    img,\n",
    "    verts_[20].cpu().numpy(),\n",
    "    cam=orig_cam[20],\n",
    "    color=[1,0,0],\n",
    "    mesh_filename=None,\n",
    ")\n",
    "\n",
    "plt.imshow(img.astype(np.int).squeeze(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vibe-env",
   "language": "python",
   "name": "vibe-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
